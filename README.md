
# Table of Contents

1.  [10,000 Hours of Machine Learning.](#orgb855f38)
2.  [Structure of the Repository](#org13c127e)
3.  [PORTFOLIO](#org2444ac5)
    1.  [Projects](#org755d658)
4.  [Education](#org2540866)
5.  [PROFICIENCY](#org3e3dd0d)


<a id="orgb855f38"></a>

# 10,000 Hours of Machine Learning.

**note: the passion from this monolithic repository leaked into its parent subject - AI.**
**as a result [abaj.ai](https://abaj.ai) was born, and now acts as a prose-wrapper for this repository**

> "Machine Learning is just lego for adults" - Dr. Kieran Samuel Owens

> "S/he who has a why, can bear almost any how." - Friedrich Nietzche

*Why do anything else, when the thing that you could do would help with everything else?*

---

To become an expert at anything, there is a common denominator:

<div class="org-center">
<p>
10,000 hours of <b>deliberate practise</b> on the subject.
</p>
</div>


<a id="org13c127e"></a>

# Structure of the Repository

There are 3 main features:

1.  PORTFOLIO
    -   Briefly, these are solutions to **classical** problems, MNIST, Boston Housing, XOR, etc.
2.  EDUCATION
    -   These contain coursework from my university and MOOCs (that which I am allowed to share). Additionally my textbook solutions are included here.
3.  PROFICIENCY
    -   These are my more complex and non-trivial projects. They are more fun, but also more novel and thus less deterministic; Kanye West chatbot, Peg Solitare Reinforcement Learner, Ultimate Frisbee Computer Vision, etc.


<a id="org2444ac5"></a>

# PORTFOLIO

In no particular order, here are a list of the methods you will find in the notebooks. The emphasis is on understanding their limitations, benefits and constructions.

-   Least Squares Regression
-   Random Forests
-   Boosting, Bagging
-   Ensemble Methods
-   Multilayer Perceptrons
-   Naive Bayes
-   K-means regression
-   K-nearest Neighbours Clustering
-   Logistic Regression
-   Decision Trees
-   SVM
-   Kernel Methods
-   GAN's
-   Stable Diffusion
-   Recurrent Neural Networks
-   Convolutional Neural Networks
-   Transformers
-   word2vec, GLoVE and NLP
-   LLM


<a id="org755d658"></a>

## Projects

To gain proficiency in all of the above methods, I have solved classical problems that lend themselves well to that particular method:

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Dataset</th>
<th scope="col" class="org-left">Accuracy</th>
<th scope="col" class="org-left">Model</th>
</tr>
</thead>

<tbody>
<tr>
<td class="org-left">MNIST</td>
<td class="org-left">A%</td>
<td class="org-left">KNN</td>
</tr>


<tr>
<td class="org-left">FMNIST</td>
<td class="org-left">B%</td>
<td class="org-left">Random Forest</td>
</tr>


<tr>
<td class="org-left">KMNIST</td>
<td class="org-left">C%</td>
<td class="org-left">2-layer CNN</td>
</tr>


<tr>
<td class="org-left">CIFAR</td>
<td class="org-left">D%</td>
<td class="org-left">CNN</td>
</tr>


<tr>
<td class="org-left">IRIS</td>
<td class="org-left">E%</td>
<td class="org-left">SVM</td>
</tr>


<tr>
<td class="org-left">ImageNet</td>
<td class="org-left">F%</td>
<td class="org-left">ResNet50</td>
</tr>


<tr>
<td class="org-left">Sentiment140</td>
<td class="org-left">G%</td>
<td class="org-left">LSTM</td>
</tr>


<tr>
<td class="org-left">Boston Housing</td>
<td class="org-left">H%</td>
<td class="org-left">Linear Regression</td>
</tr>


<tr>
<td class="org-left">Wine Quality</td>
<td class="org-left">I%</td>
<td class="org-left">Gradient Boosting</td>
</tr>


<tr>
<td class="org-left">Pima Indians Diabetes</td>
<td class="org-left">J%</td>
<td class="org-left">Decision Tree</td>
</tr>


<tr>
<td class="org-left">IMDB Reviews</td>
<td class="org-left">K%</td>
<td class="org-left">BERT</td>
</tr>


<tr>
<td class="org-left">KDD Cup 1999</td>
<td class="org-left">L%</td>
<td class="org-left">K-Means Clustering</td>
</tr>


<tr>
<td class="org-left">Digits</td>
<td class="org-left">M%</td>
<td class="org-left">Gaussian Mixture Model</td>
</tr>


<tr>
<td class="org-left">CartPole</td>
<td class="org-left">N%</td>
<td class="org-left">Deep Q-Network</td>
</tr>
</tbody>
</table>


<a id="org2540866"></a>

# Education

For mastery, a formal education is also required; either by way of open-courseware, or by paying an institution.

I have done both, and overall benefitted as a result.

-   [X] UNSW AI
-   [X] UNSW Machine Learning and Data Mining
-   [X] UNSW Deep Learning and Neural Networks
-   [ ] UNSW Computer Vision
-   [ ] Stanford CS229 (Machine Learning)
-   [ ] Stanford CS230 (Deep Learning)
-   [ ] Mathematics for Machine Learning, Ong et al.
-   [ ] HOML (Hands on Machine Learning)
-   [ ] All of Statistics, Larry Wasserman
-   [X] Coursera Machine Learning Specialisation
-   [X] Coursera Deep Learning Specialisation


<a id="org3e3dd0d"></a>

# PROFICIENCY

To become proficient, I have applied my ML skills to solve problems of personal and social interest.

-   [X] Kanye West chatbot
-   [X] KiTS19 Grand Challenge: Kidney and Kidney Tumour Segmentation
-   [ ] Non-descriptive Ultimate Frisbee Statistics
-   [ ] OCR
-   [ ] Peg Solitaire RL

> "Read 2 papers a week" - Andrew Ng

